{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Expense Categorization using MiniLM Embeddings, SVM, XGBoost & Ensemble"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Project Overview\n\nThis notebook implements an **expense categorization system** that predicts categories such as `Food`, `Transport`, `Shopping`, etc. from raw transaction descriptions (e.g., `\"Uber ride to office\"`, `\"Starbucks latte\"`).\n\nThe pipeline uses:\n\n- **MiniLM Sentence Embeddings** (`all-MiniLM-L6-v2`) from `sentence-transformers`\n- **SVM (RBF kernel)** with hyperparameter tuning\n- **XGBoost** classifier\n- **Ensemble model** combining SVM + XGBoost probabilities\n- **Model evaluation** (accuracy, classification report, confusion matrix)\n- **Error / mistake analysis** to inspect misclassified examples\n\nThe goal is to build a **practical, accurate, and explainable** classifier suitable for a personal finance assistant or agentic AI system.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# 2. Install dependencies (run once per environment)\n!pip install -q sentence-transformers xgboost"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Imports & Configuration"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import pandas as pd\nimport numpy as np\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom xgboost import XGBClassifier\n\nimport matplotlib.pyplot as plt\n\n# Configuration\nDATA_PATH = \"expense_data.csv\"  # Update path if needed\nRANDOM_STATE = 42"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Load & Clean Dataset"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Load dataset\ndf = pd.read_csv(DATA_PATH)\n\n# Basic schema check\nassert \"Description\" in df.columns and \"Category\" in df.columns, \\    \"CSV must contain 'Description' and 'Category' columns.\"\n\n# Drop missing values\ndf = df.dropna(subset=[\"Description\", \"Category\"]).reset_index(drop=True)\n\nprint(\"Original category distribution:\")\nprint(df[\"Category\"].value_counts())\n\n# Remove extremely rare categories (< 2 samples)\ncounts = df[\"Category\"].value_counts()\nrare = counts[counts < 2].index\nprint(\"\\nDropping rare categories:\", list(rare))\n\ndf = df[~df[\"Category\"].isin(rare)].reset_index(drop=True)\n\nprint(\"\\nUpdated category distribution:\")\nprint(df[\"Category\"].value_counts())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Label Encoding & Train/Test Split"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Encode string labels -> numeric ids\nle = LabelEncoder()\ndf[\"label\"] = le.fit_transform(df[\"Category\"])\n\nX_text = df[\"Description\"].astype(str).tolist()\ny = df[\"label\"].values\n\n# Stratified train-test split to preserve label distribution\nX_train_text, X_test_text, y_train, y_test = train_test_split(\n    X_text,\n    y,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n    stratify=y\n)\n\nprint(f\"Train size: {len(X_train_text)}, Test size: {len(X_test_text)}\")\nprint(\"Classes:\", list(le.classes_))\n\n# Labels actually present in the test set (for clean reports)\ntest_labels = np.unique(y_test)\ntest_label_names = le.inverse_transform(test_labels)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. MiniLM Embeddings"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Load MiniLM sentence embedding model\nprint(\"Loading MiniLM encoder...\")\nencoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Encode train & test descriptions into dense vectors\nprint(\"\\nEncoding training descriptions...\")\nX_train_emb = encoder.encode(\n    X_train_text,\n    batch_size=32,\n    convert_to_numpy=True,\n    show_progress_bar=True\n)\n\nprint(\"\\nEncoding test descriptions...\")\nX_test_emb = encoder.encode(\n    X_test_text,\n    batch_size=32,\n    convert_to_numpy=True,\n    show_progress_bar=True\n)\n\nprint(\"\\nTrain embedding shape:\", X_train_emb.shape)\nprint(\"Test embedding shape:\", X_test_emb.shape)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Feature Scaling for SVM"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# SVM is sensitive to feature scale, so we standardize embeddings\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_emb)\nX_test_scaled = scaler.transform(X_test_emb)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Train SVM with Hyperparameter Tuning (GridSearchCV)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "svm = SVC(kernel=\"rbf\", probability=True, random_state=RANDOM_STATE)\n\nparam_grid = {\n    \"C\": [0.5, 1, 3, 5],\n    \"gamma\": [\"scale\", 0.1, 0.01]\n}\n\nprint(\"Running GridSearchCV for SVM...\")\n\ngrid_svm = GridSearchCV(\n    svm,\n    param_grid,\n    scoring=\"accuracy\",\n    cv=3,\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_svm.fit(X_train_scaled, y_train)\n\nbest_svm = grid_svm.best_estimator_\nprint(\"\\nBest SVM parameters:\", grid_svm.best_params_)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Train XGBoost Classifier"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "print(\"Training XGBoost classifier...\")\n\nxgb = XGBClassifier(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    eval_metric=\"mlogloss\",\n    random_state=RANDOM_STATE\n)\n\nxgb.fit(X_train_emb, y_train)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. Model Evaluation: SVM, XGBoost, and Ensemble"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# ---- SVM Evaluation ----\ny_pred_svm = best_svm.predict(X_test_scaled)\nacc_svm = accuracy_score(y_test, y_pred_svm)\n\nprint(\"\\n=================== SVM PERFORMANCE ===================\")\nprint(f\"Accuracy: {acc_svm:.4f}\")\nprint(classification_report(\n    y_test,\n    y_pred_svm,\n    labels=test_labels,\n    target_names=test_label_names\n))\n\n# ---- XGBoost Evaluation ----\ny_pred_xgb = xgb.predict(X_test_emb)\nacc_xgb = accuracy_score(y_test, y_pred_xgb)\n\nprint(\"\\n=================== XGBOOST PERFORMANCE ===================\")\nprint(f\"Accuracy: {acc_xgb:.4f}\")\nprint(classification_report(\n    y_test,\n    y_pred_xgb,\n    labels=test_labels,\n    target_names=test_label_names\n))\n\n# ---- Ensemble (Average Probabilities) ----\nproba_svm = best_svm.predict_proba(X_test_scaled)\nproba_xgb = xgb.predict_proba(X_test_emb)\n\nproba_ensemble = (proba_svm + proba_xgb) / 2.0\ny_pred_ens = np.argmax(proba_ensemble, axis=1)\nacc_ens = accuracy_score(y_test, y_pred_ens)\n\nprint(\"\\n=================== ENSEMBLE (SVM + XGBoost) PERFORMANCE ===================\")\nprint(f\"Accuracy: {acc_ens:.4f}\")\nprint(classification_report(\n    y_test,\n    y_pred_ens,\n    labels=test_labels,\n    target_names=test_label_names\n))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11. Confusion Matrix (Ensemble)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "cm = confusion_matrix(y_test, y_pred_ens, labels=test_labels)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(cm, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Ensemble (SVM + XGBoost)\")\nplt.colorbar()\n\ntick_marks = np.arange(len(test_label_names))\nplt.xticks(tick_marks, test_label_names, rotation=45, ha=\"right\")\nplt.yticks(tick_marks, test_label_names)\n\nthresh = cm.max() / 2.0\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        plt.text(\n            j, i, cm[i, j],\n            ha=\"center\", va=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\"\n        )\n\nplt.xlabel(\"Predicted label\")\nplt.ylabel(\"True label\")\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 12. Mistake Analysis"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "results_df = pd.DataFrame({\n    \"Description\": X_test_text,\n    \"True_label_id\": y_test,\n    \"Pred_svm_id\": y_pred_svm,\n    \"Pred_xgb_id\": y_pred_xgb,\n    \"Pred_ens_id\": y_pred_ens\n})\n\nresults_df[\"True_label\"] = le.inverse_transform(results_df[\"True_label_id\"])\nresults_df[\"Pred_svm\"] = le.inverse_transform(results_df[\"Pred_svm_id\"])\nresults_df[\"Pred_xgb\"] = le.inverse_transform(results_df[\"Pred_xgb_id\"])\nresults_df[\"Pred_ens\"] = le.inverse_transform(results_df[\"Pred_ens_id\"])\n\nmistakes = results_df[results_df[\"True_label\"] != results_df[\"Pred_ens\"]]\n\nprint(\"Number of misclassified examples (ensemble):\", len(mistakes))\nprint(\"\\nSample misclassifications:\")\nmistakes.head(15)[[\n    \"Description\", \"True_label\", \"Pred_svm\", \"Pred_xgb\", \"Pred_ens\"\n]]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 13. Helper Function for Single Prediction"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def predict_category(description: str):\n    \"\"\"Predict expense category for a single description using the ensemble model.\"\"\"\n    emb = encoder.encode([description], convert_to_numpy=True)\n    emb_scaled = scaler.transform(emb)\n    \n    proba_svm = best_svm.predict_proba(emb_scaled)\n    proba_xgb = xgb.predict_proba(emb)\n    proba_ensemble = (proba_svm + proba_xgb) / 2.0\n    \n    label_id = np.argmax(proba_ensemble, axis=1)[0]\n    return le.inverse_transform([label_id])[0]\n\n# Example\nexample = \"Starbucks coffee and sandwich\"\nprint(\"Description:\", example)\nprint(\"Predicted category:\", predict_category(example))"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}